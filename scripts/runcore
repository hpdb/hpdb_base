#!/usr/bin/env python
# -*- coding: utf-8 -*-
# To stop pipeline, create a file named stopcore at /home/hpdb/

from __future__ import division
import os
import time
import sys
import yaml
import commands
import user_management as um
import utils

sys.path.insert(1, os.environ['HPDB_BASE'] + '/scripts/tools')
import hpdb, blast, clustalo, roary, snippy, amr, vf, rast

db = um.newDBConnection()

def forcedelete(file):
    while os.path.isfile(file):
        os.remove(file)

def run():    
    # ----- Get job from queue -----
    files = sorted([x for x in os.listdir(os.environ['HPDB_BASE'] + '/queue') if os.path.isfile(x)])
    if len(files) == 0:
        return False
    
    jobid = files[0]
    job_queue_file = os.environ['HPDB_BASE'] + '/queue/' + jobid
    with open(job_queue_file, 'r') as f:
        job_dir = f.read()
    
    # Check if job really exists
    if not os.path.isdir(job_dir):
        forcedelete(job_queue_file)
        return True # return True to continue getting job from queue
    
    # ----- Load job config -----    
    os.chdir(job_dir)
    with open('configs.yaml', 'r') as f:
        configs = yaml.full_load(f)
    
    with open('running', 'w') as f:
        f.write('dumb file')
    
    try:
        if configs['jobtype'] == 'caga/vaca':
            configs = vf.run(configs)
        elif configs['jobtype'] == 'amr detection':
            configs = amr.run(configs)
        elif configs['jobtype'] == 'blast':
            configs = blast.run(configs)
        elif configs['jobtype'] == 'clustalo':
            configs = clustalo.run(configs)
        elif configs['jobtype'] == 'roary':
            configs = roary.run(configs)
        elif configs['jobtype'] == 'snippy':
            configs = snippy.run(configs)
        elif configs['jobtype'] == 'rast':
            configs = rast.run(configs)
        
        # ----- Save job config -----
        os.chdir(job_dir)
        with open('configs.yaml', 'w') as f:
            yaml.dump(configs, f)
        
        # ----- Compress -----
        if not 'external' in configs:
            os.chdir(job_dir)
            utils.zip('.', um.getUserDownloadDir(configs['userid']) + configs['jobid'] + '.zip')
    except Exception as e:
        logf = open("/home/hpdb/runcore.log", "a+")
        logf.write(str(e) + '\n')
        logf.close()
        os.chdir(job_dir)
        with open('error', 'w') as f:
            f.write(str(e))
    
    # ----- Remove job from queue -----
    forcedelete(job_queue_file)
    if not 'external' in configs:
        os.chdir(job_dir)
        forcedelete('running')
        forcedelete('queued')
    else:
        with open(os.environ['HPDB_BASE'] + '/queue/external/' + jobid, 'w') as f:
            f.write(dirpath)
    
    # ----- Move back to base folder -----
    os.chdir(os.environ['HPDB_BASE'])
    
    return True

def check_external():
    # ----- Get job from external queue -----
    files = sorted([x for x in os.listdir(os.environ['HPDB_BASE'] + '/queue/external') if os.path.isfile(x)])
    if len(files) == 0:
        return False
    
    jobid = files[0]
    job_queue_file = os.environ['HPDB_BASE'] + '/queue/external/' + jobid
    with open(job_queue_file, 'r') as f:
        job_dir = f.read()
    
    # Check if job really exists
    if not os.path.isdir(job_dir):
        forcedelete(job_queue_file)
        return True # return True to continue getting job from queue
    
    # ----- Load job config -----    
    os.chdir(job_dir)
    with open('configs.yaml', 'r') as f:
        configs = yaml.full_load(f)

def stop_if_already_running():
    script_name = os.path.basename(__file__)
    l = commands.getstatusoutput("ps aux | grep -e '%s' | grep -v grep | awk '{print $2}'| awk '{print $2}'" % script_name)
    if l[1]:
        print('Already running. Exiting...')
        db.close()
        sys.exit(0);

if __name__ == '__main__':
    stop_if_already_running()
    if not run():
        time.sleep(2)
    db.close()